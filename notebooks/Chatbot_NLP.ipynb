{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56311342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs('Chatbot-NLP/data', exist_ok=True)\n",
    "os.makedirs('Chatbot-NLP/src', exist_ok=True)\n",
    "os.makedirs('Chatbot-NLP/notebooks', exist_ok=True)\n",
    "\n",
    "# intents.json content\n",
    "intents = {\n",
    "    \"intents\": [\n",
    "        {\n",
    "            \"tag\": \"greeting\",\n",
    "            \"patterns\": [\"Hi\", \"Hello\", \"Hey\", \"Howdy\"],\n",
    "            \"responses\": [\"Hello!\", \"Hi there!\", \"Greetings!\", \"Howdy, how can I help you?\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"goodbye\",\n",
    "            \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "            \"responses\": [\"Goodbye!\", \"See you later!\", \"Have a great day!\"]\n",
    "        },\n",
    "        {\n",
    "            \"tag\": \"thanks\",\n",
    "            \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
    "            \"responses\": [\"You're welcome!\", \"No problem!\", \"Happy to help!\"]\n",
    "        }\n",
    "        # Add more intents as needed\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save intents.json\n",
    "with open('Chatbot-NLP/data/intents.json', 'w') as file:\n",
    "    json.dump(intents, file, indent=4)\n",
    "\n",
    "# train.py content\n",
    "train_script = \"\"\"\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Initialize lemmatizer and label encoder\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Load the intents file\n",
    "with open('data/intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract patterns and tags\n",
    "patterns = []\n",
    "tags = []\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        patterns.append(pattern)\n",
    "        tags.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and lower each word and remove duplicates\n",
    "words = sorted(set([lemmatizer.lemmatize(word.lower()) for pattern in patterns for word in nltk.word_tokenize(pattern)]))\n",
    "\n",
    "# Encode tags\n",
    "tags = sorted(set(tags))\n",
    "labels = label_encoder.fit_transform(tags)\n",
    "\n",
    "# Create training data\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "for i, pattern in enumerate(patterns):\n",
    "    word_list = nltk.word_tokenize(pattern)\n",
    "    word_list = [lemmatizer.lemmatize(word.lower()) for word in word_list]\n",
    "    bag = [1 if word in word_list else 0 for word in words]\n",
    "    training_sentences.append(bag)\n",
    "    training_labels.append(labels[i])\n",
    "\n",
    "training_sentences = np.array(training_sentences)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(training_sentences[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(tags), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(learning_rate=0.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
